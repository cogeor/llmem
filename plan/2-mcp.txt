# MCP Server Implementation Plan
# Component: src/mcp/
# Reference: plan/mcp_design.txt (best practices)

================================================================================
## PURPOSE
================================================================================
Primary interface to Antigravity IDE. Implements MCP protocol over stdio,
registers tools, handles requests, and returns responses with processing
instructions for the agent.

================================================================================
## MCP BEST PRACTICES (from mcp_design.txt)
================================================================================

### Architecture: LLMem follows the 3-layer pattern

1. HOST LAYER: Antigravity IDE (agent + LLM)
   - Sees few high-level tools (not 20 low-level endpoints)
   
2. ORCHESTRATOR LAYER: LLMem MCP Server
   - Exposes domain-specific tools (generate_prompt, get_artifact, etc.)
   - Handles chaining via callback pattern (prompt → host LLM → store result)
   
3. DOMAIN LAYER: Internal modules (artifact/, parser/, llm/)
   - Stateless, side-effect-contained operations
   - Don't orchestrate each other; MCP tools coordinate them

### Key Constraints Applied to LLMem

a. MINIMAL TOOL SURFACE
   - Expose ~5 focused tools, not many hyper-specific ones
   - Tools: get_artifact, list_artifacts, generate_artifact, generate_prompt, store_llm_result

b. STRUCTURED RESPONSES (not natural language)
   - All responses return JSON with: status, data, nextAction, callbackTool
   - Host agent parses structured responses to decide next steps
   - Use Zod schemas to guarantee structure

c. BOUNDED RECURSION
   - MAX_STEPS limit for chained workflows
   - Per-tool timeouts
   - Log step counts for debugging

d. SEPARATE READS FROM SIDE EFFECTS
   - Read-only: get_artifact, list_artifacts, generate_prompt
   - Side effects: store_llm_result, delete_artifact (clearly named)

e. OBSERVABILITY
   - Log each MCP call with correlation IDs
   - Track step counts in chained workflows

### Chained Call Pattern (for LLM-enhanced artifacts)

```
User: "Annotate functions in config.ts"
         │
         ▼
┌─────────────────────┐
│ Host calls:         │
│ generate_prompt     │
└─────────┬───────────┘
          │
          ▼ Returns structured response:
{
  "status": "prompt_ready",
  "promptForHostLLM": "Describe each function...",
  "callbackTool": "store_llm_result",
  "callbackArgs": { "path": "...", "task": "annotate" }
}
          │
          ▼
┌─────────────────────┐
│ Host executes       │
│ prompt with its LLM │
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ Host calls:         │
│ store_llm_result    │
│ with LLM output     │
└─────────────────────┘
```

================================================================================
## FILES & RESPONSIBILITIES
================================================================================

### server.ts
- Initialize MCP server with stdio transport
- Register all available tools
- Start server and listen for requests
- Route requests to appropriate handlers

### tools.ts
- Define tool schemas with Zod
- Implement tool logic:
  - get_codebase_info: Parse file, return structure (may return needs_llm)
  - get_artifact: Retrieve saved artifact
  - list_artifacts: Query artifact index
  - process_with_llm: Build prompt for LLM treatment
  - save_artifact: Store analysis result

### handlers.ts
- Validate requests with Zod schemas
- Format structured JSON responses
- Handle 3 response types: success, error, needs_llm

================================================================================
## MODULE INTERACTIONS
================================================================================

INTERNAL (within mcp/):
┌─────────────────┐
│   server.ts     │
│                 │
│ - startServer() │
│ - registerTools │
└────────┬────────┘
         │ dispatches to
         ▼
┌─────────────────┐      ┌─────────────────┐
│    tools.ts     │─────>│   handlers.ts   │
│                 │      │                 │
│ - generate()    │      │ - validate()    │
│ - list()        │      │ - formatResp()  │
│ - get()         │      │ - addInstr()    │
│ - delete()      │      └─────────────────┘
└────────┬────────┘
         │
         │ calls
         ▼
┌─────────────────────────────────────────┐
│            Other Modules                │
│  artifact/service  llm/client  parser/  │
└─────────────────────────────────────────┘

EXTERNAL DEPENDENCIES:
- tools.ts → Calls artifact/service.ts for CRUD operations
- tools.ts → Calls llm/client.ts for artifact generation
- tools.ts → Calls parser/extractor.ts for code structure
- server.ts → Receives Config from extension/config.ts

================================================================================
## FUTURE MODULE INTEGRATION (for artifact/, parser/, llm/)
================================================================================

When implementing the full MCP tools, import ONLY these functions from each module:

### From artifact/service.ts (Part 3)
```typescript
// Import only the service facade - NOT internal storage/index/tree modules
import {
  getArtifact,     // (path: string) => Promise<Artifact | null>
  listArtifacts,   // (options?: ListOptions) => Promise<ArtifactMetadata[]>
  saveArtifact,    // (artifact: Artifact) => Promise<void>
  deleteArtifact,  // (path: string) => Promise<boolean>
} from '../artifact/service';
```

### From parser/extractor.ts (Part 4)
```typescript
// Import only the extractor facade - NOT internal parser/outline modules
import {
  extractCodeStructure,  // (path: string) => Promise<CodeStructure>
} from '../parser/extractor';
```

### From llm/prompt-builder.ts (Part 5)
```typescript
// Import only the prompt builder - NOT templates directly
import {
  buildPrompt,     // (task: TaskType, codeStructure: CodeStructure) => string
  parseResult,     // (llmOutput: string, task: TaskType) => ParsedResult
} from '../llm/prompt-builder';
```

### Integration Pattern in tools.ts

Current stub implementation:
```typescript
// STUB: Returns placeholder data
async function handleGetArtifact(args: GetArtifactArgs): Promise<McpResponse<Artifact>> {
  return formatResponse('success', { path: args.path, content: 'STUB', metadata: {} });
}
```

After artifact/ module is implemented:
```typescript
// REAL: Calls artifact service
import { getArtifact } from '../artifact/service';

async function handleGetArtifact(args: GetArtifactArgs): Promise<McpResponse<Artifact>> {
  const artifact = await getArtifact(args.path);
  if (!artifact) {
    return formatResponse('error', undefined, `Artifact not found: ${args.path}`);
  }
  return formatResponse('success', artifact);
}
```

================================================================================
## INTERFACES (using Zod per best practices)
================================================================================

```typescript
import { z } from 'zod';

// Tool input schemas (validated with Zod)
const GenerateArtifactSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.enum(['parse', 'annotate', 'summarize']).describe("Task type"),
  options: z.object({
    focusOn: z.array(z.string()).optional(),
  }).optional(),
});

const GetArtifactSchema = z.object({
  path: z.string().describe("Source file path"),
});

const ListArtifactsSchema = z.object({
  directory: z.string().optional().describe("Filter by directory"),
  type: z.string().optional().describe("Filter by artifact type"),
});

const GeneratePromptSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.enum(['annotate', 'summarize', 'document', 'test-plan']),
});

const StoreLlmResultSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.string().describe("Task that generated this result"),
  result: z.string().describe("LLM output to store"),
});

// Structured response format (per best practices - JSON, not natural language)
interface McpResponse<T> {
  status: 'success' | 'error' | 'prompt_ready';
  data?: T;
  error?: string;
  
  // For chained calls (when status === 'prompt_ready')
  promptForHostLLM?: string;        // Prompt for host agent to execute
  callbackTool?: string;            // Tool to call with LLM result
  callbackArgs?: Record<string, unknown>;  // Arguments for callback
}
```

================================================================================
## IMPLEMENTATION ORDER
================================================================================

1. handlers.ts
   - Request validation utilities
   - Response formatting
   - Agent instruction templates

2. server.ts
   - MCP protocol setup (stdio)
   - Tool registration framework
   - Request routing

3. tools.ts
   - Tool schema definitions
   - Tool implementations (integrate with artifact/llm/parser)

================================================================================
## DEPENDENCIES
================================================================================

External packages:
- @modelcontextprotocol/sdk (MCP protocol implementation)

Internal dependencies:
- Depends on: extension/config, artifact/service, llm/client, parser/extractor
- Depended on by: extension/extension.ts (starts this server)

================================================================================
## TESTING
================================================================================

### Unit Tests (mcp/)

handlers.ts:
- validate() rejects malformed requests
- validate() accepts valid tool inputs
- formatResponse() includes agentInstructions
- formatResponse() handles errors properly

server.ts:
- startServer() initializes stdio transport
- registerTools() registers all 4 tools
- Server responds to MCP handshake

tools.ts:
- Tool schemas match expected input types
- Each tool function signature is correct

### Integration Tests (mcp/ ↔ other modules)

MCP ↔ Artifact Service:
- Test: generate_artifact calls artifact/service.createArtifact()
- Test: list_artifacts calls artifact/service.listArtifacts()
- Test: get_artifact calls artifact/service.getArtifact()
- Test: delete_artifact calls artifact/service.deleteArtifact()
- Test: Artifact errors propagate correctly to MCP response

MCP ↔ LLM Prompt Builder:
- Test: process_with_llm returns valid prompt structure
- Test: save_artifact stores LLM result correctly

MCP ↔ Parser:
- Test: generate_artifact uses parser/outline for context
- Test: Parser failure falls back to no-outline mode

MCP ↔ Extension Config:
- Test: Server reads API key from config
- Test: Server uses configured model settings

### Module Compatibility Tests

Full Flow Tests:
- Test: generate_artifact end-to-end (MCP → Parser → LLM → Artifact)
- Test: list_artifacts returns correct tree structure
- Test: Agent instructions format is understood by mock client
- Test: Error responses include proper MCP error codes

================================================================================
## ENGINEER - MANUAL TESTING
================================================================================

### Option A: Test as Standalone MCP Server (Recommended)

Antigravity calls the server directly via mcp.json.

1. Compile the project:
   ```
   npm run compile
   ```

2. Add server config to Antigravity's mcp_config.json:
   Location: %USERPROFILE%\.gemini\antigravity\mcp_config.json
   
   Via UI: Agent panel → ... → MCP Servers → Manage MCP Servers → View raw config
   
   ```json
   {
     "llmem": {
       "command": "node",
       "args": ["C:/Users/costa/src/llmem/dist/mcp/server.js"]
     }
   }
   ```
   
   Note: Use absolute path in args since there's no cwd option.

3. Refresh MCP servers in Antigravity (via MCP Servers menu or restart)

4. Verify in Output panel:
   - View → Output → Select "MCP" from dropdown
   - Look for "llmem" server startup messages

5. Test via chat:
   - Ask: "Use the llmem list_artifacts tool"
   - Ask: "Call get_artifact for src/extension/config.ts"

Dev Cycle (after code changes):
   1. npm run compile
   2. Restart Antigravity
   3. Test via chat

--------------------------------------------------------------------------------

### Option B: Test as VS Code Extension (F5)

Extension activates and starts MCP server internally.

1. Open llmem project in Antigravity

2. Press F5 to launch Extension Development Host

3. Check Output panel in host window:
   - Select "LLMem" from Output dropdown
   - Verify: "MCP server started successfully"

4. Use the status command:
   - Ctrl+Shift+P → "LLMem: Show Status"
   - Should show: "MCP Server: Running"

Dev Cycle (after code changes):
   1. npm run compile
   2. Press Ctrl+Shift+F5 to restart Extension Development Host
   3. Check Output panel for startup logs

Note: In extension mode, MCP tools may not be directly accessible via chat
unless the extension registers them with Antigravity's MCP system.

--------------------------------------------------------------------------------

### Troubleshooting

Server exits immediately:
- Check mcp.json has correct absolute path in "cwd"
- Run `node dist/mcp/server.js` manually to see error output

"Server exited before responding to initialize":
- Build may be stale: run `npm run compile`
- Server crashed during startup: check for import/require errors

No MCP tools visible in chat:
- Restart Antigravity after editing mcp.json
- Check Output → MCP for server registration status
