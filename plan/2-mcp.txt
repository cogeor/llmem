# MCP Server Implementation Plan
# Component: src/mcp/
# Reference: plan/mcp_design.txt (best practices)

================================================================================
## PURPOSE
================================================================================
Primary interface to Antigravity IDE. Implements MCP protocol over stdio,
registers tools, handles requests, and returns responses with processing
instructions for the agent.

================================================================================
## MCP BEST PRACTICES (from mcp_design.txt)
================================================================================

### Architecture: LLMem follows the 3-layer pattern

1. HOST LAYER: Antigravity IDE (agent + LLM)
   - Sees few high-level tools (not 20 low-level endpoints)
   
2. ORCHESTRATOR LAYER: LLMem MCP Server
   - Exposes domain-specific tools (generate_prompt, get_artifact, etc.)
   - Handles chaining via callback pattern (prompt → host LLM → store result)
   
3. DOMAIN LAYER: Internal modules (artifact/, parser/, llm/)
   - Stateless, side-effect-contained operations
   - Don't orchestrate each other; MCP tools coordinate them

### Key Constraints Applied to LLMem

a. MINIMAL TOOL SURFACE
   - Expose ~5 focused tools, not many hyper-specific ones
   - Tools: get_artifact, list_artifacts, generate_artifact, generate_prompt, store_llm_result

b. STRUCTURED RESPONSES (not natural language)
   - All responses return JSON with: status, data, nextAction, callbackTool
   - Host agent parses structured responses to decide next steps
   - Use Zod schemas to guarantee structure

c. BOUNDED RECURSION
   - MAX_STEPS limit for chained workflows
   - Per-tool timeouts
   - Log step counts for debugging

d. SEPARATE READS FROM SIDE EFFECTS
   - Read-only: get_artifact, list_artifacts, generate_prompt
   - Side effects: store_llm_result, delete_artifact (clearly named)

e. OBSERVABILITY
   - Log each MCP call with correlation IDs
   - Track step counts in chained workflows

### Chained Call Pattern (for LLM-enhanced artifacts)

```
User: "Annotate functions in config.ts"
         │
         ▼
┌─────────────────────┐
│ Host calls:         │
│ generate_prompt     │
└─────────┬───────────┘
          │
          ▼ Returns structured response:
{
  "status": "prompt_ready",
  "promptForHostLLM": "Describe each function...",
  "callbackTool": "store_llm_result",
  "callbackArgs": { "path": "...", "task": "annotate" }
}
          │
          ▼
┌─────────────────────┐
│ Host executes       │
│ prompt with its LLM │
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ Host calls:         │
│ store_llm_result    │
│ with LLM output     │
└─────────────────────┘
```

================================================================================
## FILES & RESPONSIBILITIES
================================================================================

### server.ts
- Initialize MCP server with stdio transport
- Register all available tools
- Start server and listen for requests
- Route requests to appropriate handlers

### tools.ts
- Define tool schemas with Zod
- Implement tool logic:
  - get_codebase_info: Parse file, return structure (may return needs_llm)
  - get_artifact: Retrieve saved artifact
  - list_artifacts: Query artifact index
  - process_with_llm: Build prompt for LLM treatment
  - save_artifact: Store analysis result

### handlers.ts
- Validate requests with Zod schemas
- Format structured JSON responses
- Handle 3 response types: success, error, needs_llm

================================================================================
## MODULE INTERACTIONS
================================================================================

INTERNAL (within mcp/):
┌─────────────────┐
│   server.ts     │
│                 │
│ - startServer() │
│ - registerTools │
└────────┬────────┘
         │ dispatches to
         ▼
┌─────────────────┐      ┌─────────────────┐
│    tools.ts     │─────>│   handlers.ts   │
│                 │      │                 │
│ - generate()    │      │ - validate()    │
│ - list()        │      │ - formatResp()  │
│ - get()         │      │ - addInstr()    │
│ - delete()      │      └─────────────────┘
└────────┬────────┘
         │
         │ calls
         ▼
┌─────────────────────────────────────────┐
│            Other Modules                │
│  artifact/service  llm/client  parser/  │
└─────────────────────────────────────────┘

EXTERNAL DEPENDENCIES:
- tools.ts → Calls artifact/service.ts for CRUD operations
- tools.ts → Calls llm/client.ts for artifact generation
- tools.ts → Calls parser/extractor.ts for code structure
- server.ts → Receives Config from extension/config.ts

================================================================================
## INTERFACES (using Zod per best practices)
================================================================================

```typescript
import { z } from 'zod';

// Tool input schemas (validated with Zod)
const GenerateArtifactSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.enum(['parse', 'annotate', 'summarize']).describe("Task type"),
  options: z.object({
    focusOn: z.array(z.string()).optional(),
  }).optional(),
});

const GetArtifactSchema = z.object({
  path: z.string().describe("Source file path"),
});

const ListArtifactsSchema = z.object({
  directory: z.string().optional().describe("Filter by directory"),
  type: z.string().optional().describe("Filter by artifact type"),
});

const GeneratePromptSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.enum(['annotate', 'summarize', 'document', 'test-plan']),
});

const StoreLlmResultSchema = z.object({
  path: z.string().describe("Source file path"),
  task: z.string().describe("Task that generated this result"),
  result: z.string().describe("LLM output to store"),
});

// Structured response format (per best practices - JSON, not natural language)
interface McpResponse<T> {
  status: 'success' | 'error' | 'prompt_ready';
  data?: T;
  error?: string;
  
  // For chained calls (when status === 'prompt_ready')
  promptForHostLLM?: string;        // Prompt for host agent to execute
  callbackTool?: string;            // Tool to call with LLM result
  callbackArgs?: Record<string, unknown>;  // Arguments for callback
}
```

================================================================================
## IMPLEMENTATION ORDER
================================================================================

1. handlers.ts
   - Request validation utilities
   - Response formatting
   - Agent instruction templates

2. server.ts
   - MCP protocol setup (stdio)
   - Tool registration framework
   - Request routing

3. tools.ts
   - Tool schema definitions
   - Tool implementations (integrate with artifact/llm/parser)

================================================================================
## DEPENDENCIES
================================================================================

External packages:
- @modelcontextprotocol/sdk (MCP protocol implementation)

Internal dependencies:
- Depends on: extension/config, artifact/service, llm/client, parser/extractor
- Depended on by: extension/extension.ts (starts this server)

================================================================================
## TESTING
================================================================================

### Unit Tests (mcp/)

handlers.ts:
- validate() rejects malformed requests
- validate() accepts valid tool inputs
- formatResponse() includes agentInstructions
- formatResponse() handles errors properly

server.ts:
- startServer() initializes stdio transport
- registerTools() registers all 4 tools
- Server responds to MCP handshake

tools.ts:
- Tool schemas match expected input types
- Each tool function signature is correct

### Integration Tests (mcp/ ↔ other modules)

MCP ↔ Artifact Service:
- Test: generate_artifact calls artifact/service.createArtifact()
- Test: list_artifacts calls artifact/service.listArtifacts()
- Test: get_artifact calls artifact/service.getArtifact()
- Test: delete_artifact calls artifact/service.deleteArtifact()
- Test: Artifact errors propagate correctly to MCP response

MCP ↔ LLM Prompt Builder:
- Test: process_with_llm returns valid prompt structure
- Test: save_artifact stores LLM result correctly

MCP ↔ Parser:
- Test: generate_artifact uses parser/outline for context
- Test: Parser failure falls back to no-outline mode

MCP ↔ Extension Config:
- Test: Server reads API key from config
- Test: Server uses configured model settings

### Module Compatibility Tests

Full Flow Tests:
- Test: generate_artifact end-to-end (MCP → Parser → LLM → Artifact)
- Test: list_artifacts returns correct tree structure
- Test: Agent instructions format is understood by mock client
- Test: Error responses include proper MCP error codes
